{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQTE4a6fSIdN",
        "outputId": "1dd7f83a-5f10-457d-8fa2-34f31110a76b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start backpropagation hidden weights:   \n",
            "[0.92383606 0.33057859] [0.38388677 0.05741121]\n",
            "Start backpropagation output weights:   \n",
            "[0.19388584] [0.16914016]\n",
            "\n",
            "End backpropagation hidden weights: \n",
            "[2.35517775 0.58205435] [ 2.25137282 -0.16576431]\n",
            "End backpropagation output weights: \n",
            "[1.52103141] [-0.68807691]\n",
            "\n",
            "Non-formatted output from neural network: \n",
            "[0.40174224] [0.55307138] [0.53311144] [0.56368397]\n",
            "Formatted and rounded answer from the neural network: \n",
            "╔                                           ╗\n",
            "║     X1          X2                Y       ║\n",
            "║                                           ║\n",
            "║     0           0                0.0      ║\n",
            "║                                           ║\n",
            "║     0           1                1.0      ║\n",
            "║                                           ║\n",
            "║     1           0                1.0      ║\n",
            "║                                           ║\n",
            "║     1           1                1.0      ║\n",
            "╚                                           ╝\n"
          ]
        }
      ],
      "source": [
        "# BACKPROPAGATION ALGORITHM GB430\n",
        "# Using:\n",
        "# One input layer with two nodes: X1 and X2\n",
        "# One hidden layer with two nodes\n",
        "# One output layer with one node.\n",
        "# Sigmoid neuron as node\n",
        "\n",
        "# Run the program by running this class\n",
        "\n",
        "import numpy as np\n",
        "#import pandas as pd\n",
        "#from pandas import DataFrame\n",
        "\n",
        "\n",
        "# Sigmoid function (used in forward propagation)\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "# Derivative of Sigmoid function (used in backward propagation)\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "\n",
        "# Desired inputs of XOR-gate\n",
        "xor_input = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "# Desired output of XOR-gate\n",
        "xor_output = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Can adjust number of epochs\n",
        "# One epoch: when the entire dataset is passed forward and backward through the neural network\n",
        "# The more epochs the closer we get to the desired XOR-gate output (but takes more time)\n",
        "epochs = 2000\n",
        "learning_rate = 0.1\n",
        "inputLayerNeurons, hiddenLayerNeurons, outputLayerNeurons = 2, 2, 1\n",
        "\n",
        "# Random weights and biases\n",
        "hidden_weights = np.random.uniform(size=(inputLayerNeurons, hiddenLayerNeurons))\n",
        "hidden_bias = np.random.uniform(size=(1, hiddenLayerNeurons))\n",
        "out_weights = np.random.uniform(size=(hiddenLayerNeurons, outputLayerNeurons))\n",
        "out_bias = np.random.uniform(size=(1, outputLayerNeurons))\n",
        "\n",
        "print(\"Start backpropagation hidden weights:   \\n\", end='')\n",
        "print(*hidden_weights)\n",
        "print(\"Start backpropagation output weights:   \\n\", end='')\n",
        "print(*out_weights)\n",
        "\n",
        "\n",
        "# Training algorithm\n",
        "for _ in range(epochs):\n",
        "    # Forward Propagation\n",
        "    hidden_layer_activation = np.dot(xor_input, hidden_weights)\n",
        "    hidden_layer_activation += hidden_bias\n",
        "    hidden_layer_output = sigmoid(hidden_layer_activation)\n",
        "\n",
        "    out_layer_activation = np.dot(hidden_layer_output, out_weights)\n",
        "    out_layer_activation += out_bias\n",
        "    predicted_out = sigmoid(out_layer_activation)\n",
        "\n",
        "    # Backpropagation\n",
        "    error = xor_output - predicted_out\n",
        "    b_predicted_output = error * sigmoid_derivative(predicted_out)\n",
        "\n",
        "    error_hidden_layer = b_predicted_output.dot(out_weights.T)\n",
        "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
        "\n",
        "    # Updating weights and biases\n",
        "    out_weights += hidden_layer_output.T.dot(b_predicted_output) * learning_rate\n",
        "    out_bias += np.sum(b_predicted_output, axis=0, keepdims=True) * learning_rate\n",
        "    hidden_weights += xor_input.T.dot(d_hidden_layer) * learning_rate\n",
        "    hidden_weights += xor_input.T.dot(d_hidden_layer) * learning_rate\n",
        "    hidden_bias += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "\n",
        "# Function for printing out results in a XOR-matrix\n",
        "def draw_xor_results():\n",
        "    output_format = np.around(predicted_out, decimals=0)\n",
        "    output_format = output_format.flatten()\n",
        "    print(\"Formatted and rounded answer from the neural network: \")\n",
        "    print('\\u2554' + \"                                           \" + u'\\u2557')\n",
        "    print('\\u2551' + \"     X1     \" + \"     X2     \" + \"           Y       \" + '\\u2551')\n",
        "    print('\\u2551' + \"                                           \" + u'\\u2551')\n",
        "    print('\\u2551' + \"     0      \" + \"     0               \", output_format[0], \"     \"+'\\u2551')\n",
        "    print('\\u2551' + \"                                           \" + u'\\u2551')\n",
        "    print('\\u2551' + \"     0      \" + \"     1               \", output_format[1], \"     \"+'\\u2551')\n",
        "    print('\\u2551' + \"                                           \" + '\\u2551')\n",
        "    print('\\u2551' + \"     1      \" + \"     0               \", output_format[2], \"     \"+'\\u2551')\n",
        "    print('\\u2551' + \"                                           \" + u'\\u2551')\n",
        "    print('\\u2551' + \"     1      \" + \"     1               \", output_format[3], \"     \"+'\\u2551')\n",
        "    print('\\u255a' + \"                                           \" + '\\u255d')\n",
        "\n",
        "\n",
        "print(\"\\nEnd backpropagation hidden weights: \\n\", end='')\n",
        "print(*hidden_weights)\n",
        "print(\"End backpropagation output weights: \\n\", end='')\n",
        "print(*out_weights)\n",
        "\n",
        "print(\"\\nNon-formatted output from neural network: \\n\", end='')\n",
        "print(*predicted_out)\n",
        "\n",
        "draw_xor_results()"
      ]
    }
  ]
}